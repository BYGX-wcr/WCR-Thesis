%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

%%
%% 使用 njuthesis 文档类生成南京大学本科生毕业论文的示例文档
%% 
%%

%% 
%% 南京大学本科学位论文模板
%% 2018年封面，摘要都发生了变化，本模板由以下2016年模板更改而来：http://haixing-hu.github.io/nju-thesis/

%% 如需Adobe字体请用（默认）
%\documentclass[adobefonts]{njuthesis}
%% MacOS系统请用
%\documentclass[macfonts]{njuthesis}
%% Windows系统请用
\documentclass[winfonts]{njuthesis}
%% Linux系统请用
%\documentclass[linuxfonts]{njuthesis}

% 添加额外的包
\usepackage{listings}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的中文封面
% 论文标题
\title{分析和仿真带权拥塞控制算法}

% 论文作者姓名
\author{吴昌容}
% 论文作者学号
\studentid{161220134}
% 导师姓名职称
\supervisor{田臣}
% 导师职称
\supervisortitle{副教授}
% 论文作者院系
\department{计算机科学与技术系}
% 论文作者专业方向
\major{计算机科学与技术}
% 论文作者的年级
\grade{2016级}
% 论文提交日期，需设置年、月、日。此属性可选，默认值为最后一次编译时的日期，精确到日。
\submitdate{2020年5月22日}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的英文封面

% 论文的英文标题
\englishtitle{Simulation and Analysis of Weighted Congestion Control Algorithms}
% 论文作者姓名的拼音
\englishauthor{Changrong Wu}
% 导师姓名职称的英文
\englishsupervisor{Associate Professor Chen Tian}
% 论文作者所在院系的英文名称
\englishdepartment{Department of Computer Science and Technology}
% 论文作者所在学校或机构的英文名称。此属性可选，默认值为``Nanjing University''。
\englishinstitute{Nanjing University}
% 论文完成日期的英文形式，默认最后一次编译的时间
\englishdate{May 22, 2020}
% 专业
\englishinstitute{Computer Science and Technology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的页眉页脚
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\bfseries 161220134 }
\chead{分析和仿真带权拥塞控制算法}
\rhead{吴昌容}
\renewcommand{\headrulewidth}{0.4pt}
%\renewcommand{\footrulewidth}{0.4pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

% 制作中文封面
\maketitle
% 制作英文封面
% \makeenglishtitle
% 毕业论文过程管理四页表
%\controlpage %可以将word文件交给老师签字后扫描转成pdf，然后命名为controlpage.pdf

% 论文的中文摘要
\begin{abstract}
  拥塞控制是传输层协议TCP（Transmission Control Protocol）的核心组成部分。一般的拥塞控制算法追求公平性，即希望能在每一条参与竞争的网络流中平均分配带宽。而带权拥塞控制（又称加权拥塞控制）算法则是普通拥塞控制算法的拓展，其试图用端到端的方式来在TCP流中按流的权重来分配带宽。计算机网络界的前辈们已经提出了两种著名的带权拥塞控制算法——MulTCP和EWTCP。由于基于加增乘减的拥塞控制算法的公平性最易于掌控，所以这两种算法均是希望通过调整TCP流的加增乘减机制来实现加权公平性。然而在科研实验中，我发现这些经典的加权拥塞控制算法并不总是能很好地按权重来分配带宽。事实上，在不同的环境条件下它们的表现会出现极大的区别。在本论文中，我将会呈现MulTCP和EWTCP两种加权拥塞控制算法的仿真实验结果，并分析其行为和性能变化。我会用实验数据说明加权拥塞控制算法的性能实际上会受到交换机缓冲区大小和链路传播时延的严重影响。最后，我总结出了这两种加权拥塞控制算法的表现随环境因素变化的规律。
% 同时应该注意到，空白页是故意留白，以便章节开头能够出现在偶数页。
% 中文关键词。关键词之间用中文全角分号隔开，末尾无标点符号。
\keywords{拥塞控制；带宽分配；加权公平性}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 论文的英文摘要
\begin{englishabstract}
  Congestion Control is one of the critical components of TCP (Transmission Control Protocol). Normal congestion control algorithms aim to achieve fairness among all competing flows, which means they usually try to allocate a fair share of the link capacity to each flow. Weighted congestion control algorithms are extensions of the normal versions. Basically speaking, weighted congestion control algorithms aim at using end-to-end mechanisms to allocate bandwidth proportionally according to flows’ weights. MulTCP and EWTCP are two representative schemes of weighted congestion control algorithms. Since the fairness of AIMD-based congestion control algorithms is most comprehensible, both of the schemes attempt to achieve weighted proportionality via modifying the behavior of AIMD (Additive Increase Multiplicative Decrease). However, in real experiments, I find that those weighted congestion control algorithms have variable performance in different circumstances. In fact, their performance with regard to weighted proportionality fluctuates drastically in various network environments. In this paper, I present the simulation results of MulTCP as well as EWTCP and analyze the variation of their behaviors and performance. I show that both switch buffer size and propagation delay can significantly affect the performance of AIMD-based weighted congestion control algorithms, while these environmental parameters do not influence flows’ behaviors. Finally, I conclude the pattern of weighted congestion control algorithms’ performance variation. 
% 英文关键词。关键词之间用英文半角逗号隔开，末尾无符号。
\englishkeywords{Congestion Control, Bandwidth Allocation, Weighted Fairness}
\end{englishabstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 论文的前言，应放在目录之前，中英文摘要之后
%
% \begin{preface}

% 这是对我本科科研成果的一个提炼和总结。

% \vspace{1cm}
% \begin{flushright}
% 吴昌容
% 2020年5月22日于广西南宁
% \end{flushright}

% \end{preface}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成论文目录
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成插图清单。如无需插图清单则可注释掉下述语句。
%\listoffigures

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成附表清单。如无需附表清单则可注释掉下述语句。
%\listoftables

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 开始正文部分
\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 学位论文的正文应以《绪论》作为第一章
\chapter{绪论}\label{chapter:intro}

拥塞控制（Congestion Control）是传输层协议TCP（Transmission Control Protocol）的核心组成部分。
在TCP的整体控制系统中，拥塞控制担负着解决网络拥塞和避免丢包以提升网络性能的重要责任\cite{jacobson1988congestion}。
也正是由于拥塞控制对于网络流的行为和表现有着重大的影响，所以拥塞控制一直是网络研究中的重点话题。
公平性是拥塞控制算法的主要性质之一，其要求拥塞控制算法能够在互相竞争的网络流中公平地分配带宽，即每一条TCP流都能均等地分到拥塞链路的资源。
公平性不仅是个技术问题还是个社会问题，它关乎到每一个网络用户（对于互联网来说，就是地球上的每一个普通人）的利益。
因此，公平性从传输层控制协议（TCP）加入拥塞控制算法伊始，就一直为人们所关注。
在拥塞控制算法和理论发展的早期，人们主要关注如何才能使TCP拥塞控制算法达到公平性以及某种拥塞控制算法是否满足公平性的要求等问题\cite{chiu1989analysis}\cite{kelly1998rate}\cite{hasegawa1999fairness}。
但是随着网络的发展，在某些新的应用场景\cite{wischik2011design}\cite{popa2012faircloud}\cite{Nathan2019wcubic}下，人们其实并不是希望公平地分配带宽而是希望能按权重来分配带宽。
因此，研究者们又进一步关注到了公平性的广义拓展——加权公平性。
加权公平性要求拥塞控制算法能够在互相竞争的网络流中按照它们的权重来分配带宽，即每一条TCP流都能分到对应于它权重的拥塞链路的资源。
具有加权公平性的拥塞控制算法就是带权拥塞控制或加权拥塞控制（Weighted Congestion Control）算法，这种算法根据每条网络流的权重来分配带宽。
由于基于加增乘减机制的拥塞控制算法的公平性最易于掌控和理解，最经典的两种带权拥塞控制算法MulTCP\cite{crowcroft1998differentiated}和EWTCP\cite{wischik2011design}也都是基于加增乘减机制的。
加增乘减（Additive Increase Multiplicative Decrease）机制是TCP协议拥塞控制的主要方法，它结合了线性探索和乘性退避的功能。
而且，加增乘减机制已经被证明可以保证每条参与竞争的TCP流都一定会收敛到公平的均衡点~\cite{chiu1989analysis}。
本文也将主要关注MulTCP和EWTCP这两种经典的基于加增乘减机制的带权拥塞控制算法。

如今网络服务已经由free-use模型发展到pay-for-use模型，传统的公平性应当增强为加权公平性，以实现基于不同价格的差异化服务。
首先，为了支持日渐增多的异质化应用对网络带宽的不同要求，网络管理者往往需要在网络中为不同的流量配置优先级或权重\cite{Hong2013SWAN}。
拥有较高权重的流在竞争有限的带宽资源时，应该比权重较低的流获得更多的带宽。
其次，在数据中心中网络带宽应该按照每个租户支付的金钱来进行分配\cite{popa2012faircloud}。
如果一个租户支付了更高的服务费用，那么它理应获得更多的带宽。
带权拥塞控制正是一种可以用端到端的方式高效地实现差异化服务的机制。
而且带权拥塞控制还可以从最细粒度的层级上去控制带宽，并具有工作保留（Work-Conserving）的特性。
比如，如果能用带权拥塞控制来实现Faircloud\cite{popa2012faircloud}中的PS-N、PS-L和PS-P带宽分配系统，那么将极大地提升系统的可拓展性，并降低交换机队列资源的消耗。
Minerva\cite{Nathan2019wcubic}中也正是使用带权拥塞控制来动态地控制每一条流的带宽，并辅以优化最终实现了更高粒度的用户体验质量（Quality of Experience）层级的公平性。
MPTCP\cite{wischik2011design}中也使用带权拥塞控制来控制每条路径上的每条子流的速率，以最终达到统一的聚合流的公平性。

现有的支持网络带宽差异化分配的工作主要是利用交换机上的队列资源来对流量进行调度从而实现按权或按优先级分配带宽，比如Weighted Fair Queuing （WFQ）\cite{demers1989analysis}\cite{Abhay1993WFQ}和Diffserv\cite{Kathleen1998Diffserv}。
前几年提出的NUMFabric\cite{nagaraj2016numfabric}和Faircloud\cite{popa2012faircloud}均依赖于交换机上的WFQ来实现加权公平性。
功能日渐强大的交换机确实可以帮助实现加权公平性，但对交换机的依赖也使得这些系统的部署受到了限制。
由于网络节点总是要对网络的反馈作出回应，因此以加权拥塞控制为代表的端到端速率调整机制其实对于实现加权公平性更为普适。
MulTCP\cite{crowcroft1998differentiated}和EWTCP\cite{wischik2011design}就是两种典型的基于AIMD的加权拥塞控制算法。
它们均通过在端上修改TCP拥塞控制的加增乘减机制来实现差异化带宽分配。
MulTCP根据锯齿模型分析得出的结论\cite{Floyd1997Sawtooth}提出：对于权重为$w$的TCP流，其加增参数和乘减参数应当分别为$w$和$1-\frac{1}{2w}$。
事实上MulTCP是存在问题的，因为其理论模型是基于每一条流拥塞窗口的变化均是高度同步这一假设的，而这个假设是不符合实际的。
EWTCP的设计想法则是来自于TCP流带宽随机测量模型\cite{padhye1998modeling}，对于权重为$w$的TCP流，它在普通加增乘减的基础上将加增的参数改为$w^2$。
EWTCP使用的模型在特定的情形下确实能够对单条TCP流的带宽进行较为准确地测量，但是这种模型没有将网络中动态变化的排队时延考虑进去。
因此，当交换机中队列的长度发生改变时，这种模型就会与现实产生偏差。
总的来说，MulTCP和EWTCP都只是提出了一种可能的加权拥塞控制算法，而这些算法都没有能够完美地实现加权公平性。
这主要是因为之前的这两份工作\cite{crowcroft1998differentiated}\cite{wischik2011design}都忽视了交换机缓冲区大小、传播时延和ACK选项对不同权重的流的影响。

本文旨在对几种代表性的加权拥塞控制算法进行实验和分析，以期总结出基于AIMD的加权拥塞控制算法的一些特性和规律，并最终提升人们对于加权拥塞控制算法的理解，帮助网络工程师们了解如何在实践中应用加权拥塞控制。本文的主要工作如下：
\begin{enumerate}
\item 对几种典型带权拥塞控制算法进行仿真和拟真实验

\item 分析典型带权拥塞控制算法的表现

\item 总结基于AIMD的带权拥塞控制算法的加权公平性与网络环境变量之间的关系。
\end{enumerate}

% \section{本文结构}
% 本文的各章节组织结构如下：

% 第一章：绪论。
% 第二章：经典带权拥塞控制
% 第三章：实验与分析
% 第四章：讨论与启示
% 第五章：相关工作
% 第六章：总结

\chapter{带权拥塞控制}\label{chapter:wcc}

在加权公平性模型下，当两条权重分别为$a_1, a_2$的使用加增乘减机制的TCP流共享一个容量为$C$的瓶颈链路时，它们分到的带宽应该分别为$\frac{a_1}{a_1 + a_2}C$和$\frac{a_2}{a_1 + a_2}C$，即在稳态下它们分到的带宽应当与他们的权重成比例。
这也正是带权拥塞控制算法在该例子中应该达到的效果，而普通拥塞控制算法的效果则可以看成这两条流权重均为1时的情况。
在这一节中，我将首先回顾一下两种代表性的带权拥塞控制算法MulTCP\cite{crowcroft1998differentiated}和EWTCP\cite{wischik2011design}，并提炼出它们设计思想的核心，并给出基于AIMD的带权拥塞控制算法的基本形式。

\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.4\textwidth}
    \centering
      \includegraphics[width=1\textwidth]{MulTCP.pdf}
      \subcaption{两条共享同一链路权重分别为$a_1$和$a_2$的MulTCP流的加增乘减变化。}
      \label{fig:MulTCP}
  \end{subfigure}
  \hspace{1em}
  \begin{subfigure}{.4\textwidth}
    \centering
      \includegraphics[width=1\textwidth]{EWTCP.pdf}
      \subcaption{两条共享同一链路权重分别为$a_1$和$a_2$的EWTCP流的加增乘减变化。}
      \label{fig:EWTCP}
  \end{subfigure}
  \begin{subfigure}{.4\textwidth}
    \centering
      \includegraphics[width=1\textwidth]{WReno-AI.pdf}
      \subcaption{两条共享同一链路权重分别为$a_1$和$a_2$的WCC-AI流的加增乘减变化。}
      \label{fig:WReno-AI}
  \end{subfigure}
  \hspace{1em}
  \begin{subfigure}{.4\textwidth}
    \centering
      \includegraphics[width=1\textwidth]{WReno-MD.pdf}
      \subcaption{两条共享同一链路权重分别为$a_1$和$a_2$的WCC-MD流的加增乘减变化。}
      \label{fig:WReno-MD}
  \end{subfigure}
  \caption{几种基于加增乘减机制的典型带权拥塞控制算法运行抽象模拟图。图中每一条轴代表了一条TCP流0-1标准化后的窗口大小。蓝色实线（高效线）代表链路带宽用满，蓝色虚线（加权公平线）代表两条流权重成比例。}
\label{fig:AIMD}
\end{figure}

\section{MulTCP}

MulTCP\cite{crowcroft1998differentiated}同时修改加增乘减机制的加增（AI）部分和乘减（MD）部分以实现加权公平性。对于权重为$w$的流，其加增参数为$w$，乘减参数为$1-\frac{1}{2w}$。一个权重为$w$的MulTCP流发送端的行为可以概括为：

\begin{itemize}
  \item[*] \textbf{\large MulTCP}
  \item[-] {\bf 加增（Additive Increase）：} $cwnd \leftarrow cwnd + \frac{w}{cwnd}$ \\
  每收到一个ACK，拥塞窗口$cwnd$就增大$\frac{w}{cwnd}$。
  \item[-] {\bf 乘减（Multiplicative Decrease）:}
  $ \left\{
  \begin{aligned}
  cwnd & \leftarrow \frac{cwnd}{2} & cwnd < ssthresh \\
  cwnd & \leftarrow \left( 1 - \frac{1}{2w} \right) \cdot cwnd & otherwise\\
  \end{aligned}
  \right.$ 
  当拥塞窗口$cwnd$大小大于或等于慢启动阈值$ssthresh$时，每探测到一次丢包，窗口大小就减少$\frac{1}{2w} \cdot cwnd$。若窗口大小小于慢启动阈值，则每对于每次丢包，窗口直接减半。
\end{itemize}
现在，我来解释一下MulTCP的设计思路。如果$n$条使用加增乘减机制的普通TCP流在一条瓶颈链路上互相竞争，则每一条都会获得链路带宽的$\frac{1}{n}$。
因此，如果在该拥塞链路中一条流可以同时模拟$w$条流的行为，也就是使它成为$w$条流聚合而成的流，它将可以获得链路带宽的$\frac{w}{n}$。
基于上述直觉，MulTCP在设计中企图使一条权重为$w$的TCP流的表现与$w$条普通TCP流的聚合一样。
然而这一设想与传统加增乘减机制的直观理解\cite{chiu1989analysis}并不一致。
如图\ref{fig:MulTCP}所示，两条MulTCP流的变化轨迹并不能收敛到加权公平线上，事实上其收敛处与加权公平线相距甚远。
MulTCP的理论分析是基于传统的锯齿模型的，而锯齿模型已经被证实是极其不精确的\cite{alizadeh2011analysis}，所以其理论分析与现实是有偏差的。
此外，启发MulTCP设计的直觉——用一条流来模拟多条流的聚合其实很难在现实中实现。
其主要原因在于当多个丢包发生时，由于理应窗口减半的流的数目无法知悉，发送端很难准确地去测量多条TCP流应当减少的总拥塞窗口值。
比如，在一次拥塞中$k$条流丢了$p$个包时，我们无法在没有全局信息的情况下确定这$p$个包是分别属于这$k$条流，还是只属于1条流。
因此，即使每条流的窗口大小相同均为$cwnd$，最终我们还是无法计算出总的拥塞窗口应该减少的量到底是$\frac{k \cdot cwnd}{2}$还是$\frac{cwnd}{2}$。
我将在第三章中指出MulTCP事实上是无法准确地实现加权公平性的。

\section{EWTCP}

EWTCP仅通过修改加增（AI）部分来实现加权公平性。对于一条权重为$w$的TCP流，EWTCP将AI的参数设定为$w^2$。
一个权重为$w$的MulTCP流发送端的行为可以概括为：

\begin{itemize}
  \item[*] \textbf{\large EWTCP}
  \item[-] {\bf Additive Increase:} $cwnd \leftarrow cwnd + \frac{w^2}{cwnd}$ \\
  每收到一个ACK，拥塞窗口$cwnd$就增长$\frac{w^2}{cwnd}$。
  \item[-] {\bf Multiplicative Decrease:} $cwnd \leftarrow \frac{cwnd}{2}$ \\
  每探测到一次丢包，拥塞窗口就减半。
\end{itemize}

EWTCP又被叫做WAIMD\cite{Honda2009EWTCP}，它对AI部分的修改是源自于一个经典的TCP流带宽测量模型\cite{padhye1998modeling}。
然而，这一模型在计算单次往返时间（RTT）时并没有考虑动态的排队时延，而是使用一个静态估计的平均值来带入公式中。
现在的交换机都是配备了很大的缓冲区的\cite{Jim2012Bufferbloat}，忽视排队时延的动态变化会导致模型严重第偏离实际。
此外，模型中假设一个RTT内任意两个包之间的丢包事件是相互独立的。
在广域网的环境中，由于流的到达是随机的，所以这一假设可能成立。
但是，这一假设对于数据中心网络等私有网络环境来说是不符合实际的，因为在这种网络中，流量通常是有模式的。
EWTCP的模型在进行建模时还假设TCP流用的是累积ACK，然而现在绝大多数的环境下，TCP流都会使用选择ACK。
图\ref{fig:EWTCP}也表明了EWTCP的表现与传统的理解是不一致的，其收敛点与最优点也相差甚远。

\section{基于加增乘减的带权拥塞控制算法的基本形式}

仔细观察MulTCP和EWTCP的AIMD算法，其实可以发现它们都只是为加增乘减（AIMD的AI部分或MD部分配置了一个与流权重相关的因子。\cite{bansal2001binomial}中将包括AIMD在内的所有二项式拥塞控制算法做了一个形式化的描述。而对于基于AIMD的拥塞控制算法，我们也在此给出一个形式化的描述。对于一条权重为$w$的TCP流，基于AIMD的带权拥塞控制算法总是呈现出如下的形式：

\begin{itemize}
  \item[*] \textbf{\large General Form of Weighted AIMD Congestion Control}
  \item[-] {\bf Additive Increase:} $cwnd \leftarrow cwnd + \frac{w^\alpha}{cwnd}, \alpha \in \mathbb{R}$ \\
  每收到一个ACK，拥塞窗口$cwnd$就增长$\frac{w^\alpha}{cwnd}$。
  \item[-] {\bf Multiplicative Decrease:} $cwnd \leftarrow cwnd (1 - \frac{w^{-\beta}}{2}), \beta \in \mathbb{R}$ \\
  每探测到一次丢包，拥塞窗口就减少$\frac{w^{-\beta}}{2}$。
\end{itemize}
也就是不同的基于AIMD的带权拥塞控制算法其实只是选择了不同的$\alpha$和$\beta$参数。
基于Chiu论文\cite{chiu1989analysis}中的抽象模型来分析，其实直觉上最简单有效的带权拥塞控制算法应当为$\alpha=1,\beta=0$的WCC-AI和$\alpha=0,\beta=1$的WCC-MD。


\begin{itemize}
  \item[*] \textbf{\large WCC-AI}
  \item[-] {\bf Additive Increase:} $cwnd \leftarrow cwnd + \frac{w}{cwnd}$ \\
  每收到一个ACK，拥塞窗口$cwnd$就增长$\frac{w}{cwnd}$。
  \item[-] {\bf Multiplicative Decrease:} $cwnd \leftarrow \frac{cwnd}{2}$ \\
  每探测到一次丢包，拥塞窗口就减半。
\end{itemize}


\begin{itemize}
  \item[*] \textbf{\large WCC-MD}
  \item[-] {\bf Additive Increase:} $cwnd \leftarrow cwnd + \frac{1}{cwnd}$ \\
  每收到一个ACK，拥塞窗口$cwnd$就增长$\frac{1}{cwnd}$。
  \item[-] {\bf Multiplicative Decrease:} $cwnd \leftarrow ( 1 - \frac{1}{2w}) \cdot cwnd$ \\
  每探测到一次丢包，拥塞窗口就减少$\left( 1 - \frac{1}{2w} \right)$。
\end{itemize}

如图\ref{fig:WReno-AI}和\ref{fig:WReno-MD}中所示，WCC-AI和WCC-MD的收敛点都基本位于最优点附近。
此外，WCC-AI与WCC-MD其实可以说是最基本的两种带权拥塞控制算法，MulTCP其实可以看成WCC-AI和WCC-MD的叠加，EWTCP其实可以看成一个WCC-AI使用了真实权重的平方来作为算法权重参数。
接下来的文章中我也将会把WCC-AI和WCC-MD纳入分析。

\chapter{实验与分析}\label{chapter:analysis}

% \begin{figure*}[tp]
% 	\centering
%   \begin{subfigure}{.6\textwidth}
%     \centering
% 		\includegraphics[width=3.2in]{multcp-dynamic.pdf}
%     \caption{MulTCP流仿真带宽图}
%     \label{fig:MulTCPPerformance}
%   \end{subfigure}
%   \begin{subfigure}{.6\textwidth}
%     \centering
% 		\includegraphics[width=3.2in]{ewtcp-dynamic.pdf}
%     \caption{EWTCP流仿真带宽图}
%     \label{fig:EWTCPPerformance}
% 	\end{subfigure}
%   \begin{subfigure}{.6\textwidth}
%     \centering
% 		\includegraphics[width=3.2in]{wrenoai-dynamic.pdf}
%     \caption{WCC-AI流仿真带宽图}
%     \label{fig:WReno-AIPerformance}
%   \end{subfigure}
%   \begin{subfigure}{.6\textwidth}
%     \centering
% 		\includegraphics[width=3.2in]{wrenomd-dynamic.pdf}
%     \caption{WCC-MD流仿真带宽图}
%     \label{fig:WReno-MDPerformance}
% 	\end{subfigure}
% 	\caption{四种典型带权拥塞控制算法的仿真结果。紫、黄、红、蓝四种颜色的线分别对应于权重为4，3，2，1的TCP流。}
% 	\label{fig:Performance}
% \end{figure*}

在这章中，首先我将使用NS3仿真器\cite{NS3}对四种典型带权拥塞控制算法进行仿真，并根据仿真结果对它们进行分析。然后，我将会使用一些拟真实验来验证我分析出的一些想法。

\section{仿真实验}
\label{sec:simulation}

\begin{figure}[h]
  \centering
  \includegraphics[width=6.0in]{multcp-dynamic.pdf}
  \caption{MulTCP流仿真带宽图}
  \label{fig:MulTCPPerformance}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[width=6.0in]{ewtcp-dynamic.pdf}
  \caption{EWTCP流仿真带宽图}
  \label{fig:EWTCPPerformance}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[width=6.0in]{wrenoai-dynamic.pdf}
  \caption{WCC-AI流仿真带宽图}
  \label{fig:WReno-AIPerformance}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[width=6.0in]{wrenomd-dynamic.pdf}
  \caption{WCC-MD流仿真带宽图}
  \label{fig:WReno-MDPerformance}
\end{figure}

\textbf{环境配置：}
在NS3仿真器中，我将MulTCP，EWTCP，WCC-AI和WCC-MD实现为ns3::TcpNewReno的子类，并在仿真实验中根据需求做出相应的调用。默认情况下，链路的带宽设置为10$Gbps$，链路的传播时延设置为20$\mu s$，交换机缓冲区的大小设置为200$KBytes$，选择ACK选项也默认开启。

\textbf{带权拥塞控制算法确实可以实现差异化带宽分配，但是它们的加权公平性并不完美：}
如图\ref{fig:MulTCPPerformance}、\ref{fig:EWTCPPerformance}、\ref{fig:WReno-AIPerformance}和\ref{fig:WReno-MDPerformance}所示，即使在动态的流模式中，四种带权拥塞控制算法也都成功地实现了使权重不同的流获得不同份额的带宽，而且四种算法由一个收敛状态转换到另一个收敛状态的速度都很快。
但是，仔细检查图中各条流的带宽可以发现使用MulTCP和EWTCP的实验中，四条流的带宽比并不是等于它们的权重比的。
反倒是最基础的WCC-AI和WCC-MD的四条流的带宽比基本接近它们的权重比。
这似乎表明MulTCP和EWTCP不能达到精确的加权公平性，而WCC-AI和WCC-MD可以达到较为精确的加权公平性。
然而，我换了不同的环境参数（如：交换机缓冲区大小，链路时延等），做了多次不同的实验后，发现事实上无论是这四种带权拥塞控制算法中的哪一种，都可以在特定的网络环境下达到精确的加权公平性，但在某些网络环境条件下它们都无法达到精确的加权公平性。
这也意味着，这四种典型的基于加增乘减的带权拥塞控制算法都可以实现流级的差异化带宽分配，但是它们的加权公平性（或者说它们的带宽分配结果）是不完美的。
更为准确地说，它们的加权公平性是会随着网络环境的变化而变化的。
我将会在节\ref{sec:emulation}中更为详细地说明这一点，并给出基于AIMD的带权拥塞控制算法的加权公平性随环境变量变化的规律。
此外，从仿真结果中可以看出WCC-MD的表现与WCC-AI大致相同，所以在接下来的内容中我将重点呈现WCC-AI的实验结果，对于WCC-MD的实验结果我会在必要时再加以说明，不过我们之后的讨论与分析会一直涵盖WCC-MD。

\begin{figure}[ht]
	\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=2.8in]{bounded.pdf}
    \caption{RWND最大为131$KBytes$}
	\label{fig:BoundedCwnd}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
		\centering
    \includegraphics[width=2.8in]{unbounded.pdf}
    \caption{RWND接近无穷大}
	\label{fig:UnbounedCwnd}
  \end{subfigure}
	\caption{接收端窗口大小对加权公平性的影响.}
	\label{fig:EndHostBuffer}
\end{figure}

\textbf{接收端窗口大小也会影响带权拥塞控制的加权公平性：}
在仿真实验中，我发现不仅带宽和时延等典型的网络环境变量会影响加权公平性，接收端窗口（RWND）大小也会影响带权拥塞控制的加权公平性。
以WCC-AI为例，如图\ref{fig:EndHostBuffer}所示，我在仿真器中生成了三条权重分别为1、2、3的流并使它们在一个带宽为1$Gbps$的链路上互相竞争。
图\ref{fig:BoundedCwnd}展示了当RWND被限制在131$KBytes$时三条流的拥塞窗口（CWND）变化。
在三条流的往返时间一致的情况下，三条流的CWND的比值就等同于它们带宽的比值，也就是CWND反映了这三条流的带宽。
可以发现当RWND成为瓶颈，也就是权重较大的流的拥塞窗口会被RWND限制住时，两条权重不一样的流即使最开始获得了不同的带宽，最终也会收敛到二者带宽相等的情况。
图\ref{fig:UnbounedCwnd}展示了当我把RWND调到极大（可以理解为无穷大）时这三条流运行的情况。
可以发现三条流的运行又恢复正常了，当最终收敛时三条流的CWND值接近达到三条流的权重比$1:2:3$，即它们的带宽又满足加权公平性了。

\begin{figure}[ht!]
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{reno-cack.pdf}
    \caption{使用累积ACK的NewReno流}
    \label{fig:renoCack}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.4in]{reno-sack.pdf}
    \caption{使用选择ACK的NewReno流}
    \label{fig:renoSack}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{multcp-cack.pdf}
    \caption{使用累积ACK的MulTCP流}
    \label{fig:multcpCack}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{multcp-sack.pdf}
    \caption{使用选择ACK的MulTCP流}
    \label{fig:multcpSack}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{ewtcp-cack.pdf}
    \caption{使用累积ACK的EWTCP流}
    \label{fig:ewtcpCack}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{ewtcp-sack.pdf}
    \caption{使用选择ACK的EWTCP流}
    \label{fig:ewtcpSack}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{wreno-cack.pdf}
    \caption{使用累积ACK的WCC-AI流}
    \label{fig:wrenoCack}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{wreno-sack.pdf}
    \caption{使用选择ACK的WCC-AI流}
    \label{fig:wrenoSack}
  \end{subfigure}
	\caption{NewReno普通拥塞控制和MulTCP、EWTCP、WCC-AI三种带权拥塞控制算法与两种ACK选项的对比仿真。}
	\label{fig:CackSackDiff}
\end{figure}

\begin{table}[h]
	\caption{图\ref{fig:CackSackDiff}中5秒仿真内的各条流平均带宽比。}
	\label{tab:CackSackDiff}
	\centering
	\begin{tabular}{c|cc}
		\hline
		\multirow{2}{*}{拥塞控制算法} & \multicolumn{2}{c}{五条流的平均带宽比} \\
		\cline{2-3} & 选择ACK & 累积ACK \\
		\hline
		NewReno~\cite{Henderson2012NewReno} & 1:1.01:1.01:1.03:1.02 & 1:1.04:1.07:0.74:0.92 \\
		MulTCP~\cite{crowcroft1998differentiated} & 1:3.67:8.81:16.03:24.96 & 1:1.10:1.36:1.54:1.55 \\
		EWTCP~\cite{Honda2009EWTCP} & 1:2.38:5.30:9.37:13.21 & 1:2.81:3.15:3.28:3.53\\
		WCC-AI & 1:1.65:2.50:3.33:4.14 & 1:1.44:2.48:2.56:3.10\\
		\hline
	\end{tabular}
\end{table}

\textbf{选择ACK和累积ACK选项对带权拥塞控制算法的加权公平性也有显著影响：}
我运用仿真实验检查了普通拥塞控制的代表NewReno和MulTCP、EWTCP、WCC-AI三种基于加增乘减的带权拥塞控制算法的（加权）公平性与ACK机制的关系，结果如图\ref{fig:CackSackDiff}和表\ref{tab:CackSackDiff}所示。
我们可以观察到对于普通拥塞控制算法NewReno来说，虽然选择ACK能够减少其流的带宽波动，但是从长期来看ACK选项并没有严重地影响其公平性——当开启选择ACK时，五条流的带宽比接近$1:1:1:1:1$；当使用累积ACK时，五条流的带宽比稍有变化，但是仍基本维持在原值左右。
然而，对于带权拥塞控制算法，ACK选项则会对其加权公平性产生显著的影响。
如表\ref{tab:CackSackDiff}中第二行所示，当使用选择ACK时，权重最大的MulTCP流与权重最小的MulTCP流的带宽比接近$1:25$，而当使用累积ACK时，这一比值就降到了接近$1:1.5$。
EWTCP也有相同的表现，WCC-AI受到的影响相对没有那么剧烈，但是其加权公平性的准确度还是降低了的。
我觉得这种意料之外的影响应该是因为累积ACK会导致过多的重传事件发生\cite{Mathis1996SACK}，这会使窗口剧烈波动，从而对拥塞控制算法的稳态产生影响。
当流的拥塞控制算法基本遵循AIMD的规则时，流受到的影响比较小，如NewReno和WCC-AI的流；当流的拥塞控制算法与AIMD的规则差别较大时，其窗口变动会较为剧烈，流受到的影响就会比较大，如MulTCP和EWTCP的流。
在下一节的仿真实验中，我也会进一步地来验证这一点。

\section{拟真实验}
\label{sec:emulation}

仿真实验缺乏随机性，所以在这一节中我采用了拟真实验来更好地验证带权拥塞控制的加权公平性随环境变量变化的规律。
我使用了三台搭载Intel Xeon处理器、运行Linux 16.04 LTS系统的DELL服务器和一台Mellanox SN2700交换机来搭建了拟真实验环境。
我将MulTCP、EWTCP、WCC-AI和WCC-MD都基于原有的内核代码实现为新的内核拥塞控制模块并载入三台服务器的内核中。
我使用Iperf\cite{iperf}来产生TCP流量和调用所需的拥塞控制算法，使两条使用带权拥塞控制的TCP流通过一条10$Gbps$的拥塞链路到达接收主机上。
我使用Linux系统内核里的TCP参数{\tt net.ipv4.tcp\_sack}来控制ACK选项，以实现选择ACK和累积ACK的切换。

\textbf{内核实现：}
我基于现有Linux内核中的Reno拥塞控制模块的代码来实现了MulTCP、EWTCP、WCC-AI和WCC-MD。
这一实现是基于IETF的RFC文档\cite{allman2009CC}的标准实现，保证代码的基本正确性。
其实基于已有的使用加增乘减机制的拥塞控制代码来实现带权拥塞控制非常简单，只需要修改相应的加增部分或者乘减部分即可。
实现MulTCP即为同时把加增部分的参数乘上流的权重和乘减部分的参数除以流的权重。
把加增部分的参数改为流的权重的平方，即可实现EWTCP。
实现WCC-AI和WCC-MD则可以视为实现带权拥塞控制的基本样例，因为它们分别代表了修改加增部分和修改乘减部分的基本原则。
实现WCC-AI时，只需把拥塞控制模块中的\texttt{tcp\_cong\_avoid\_ai}函数改为如下的函数即可：
\lstset{language=C,basicstyle=\footnotesize,frame=shadowbox}
\begin{lstlisting}
static void tcp_wcc_cong_avoid_ai(struct sock *sk, u32 w, u32 acked)
{
	struct tcp_wcc *ca = inet_csk_ca(sk);
	struct tcp_sock *tp = tcp_sk(sk);

	if (tp->snd_cwnd_cnt >= w) {
		tp->snd_cwnd_cnt = 0;
		tp->snd_cwnd++;
	}

	ca->weight_acked_cnt += ca->weight * wcc_precision * acked;
	if (ca->weight_acked_cnt >= wcc_precision) {
		u32 delta = ca->weight_acked_cnt / wcc_precision;

		ca->weight_acked_cnt -= delta * wcc_precision;
		tp->snd_cwnd_cnt += delta;
	}

	if (tp->snd_cwnd_cnt >= w) {
		u32 delta = tp->snd_cwnd_cnt / w;

		tp->snd_cwnd_cnt -= delta * w;
		tp->snd_cwnd += delta;
	}
	tp->snd_cwnd = min(tp->snd_cwnd, tp->snd_cwnd_clamp);
}
\end{lstlisting}
观察上述代码可发现，只需在增加\texttt{ca->weight\_acked\_cnt}时加入权重参数\texttt{ca->weight}即可实现对AI部分的修改。
实现WCC-MD时，只需把拥塞控制模块中的\texttt{tcp\_reno\_ssthresh}函数改为如下的函数即可：
\lstset{language=C,basicstyle=\footnotesize,frame=shadowbox}
\begin{lstlisting}
static u32 tcp_wreno_md_ssthresh(struct sock *sk)
{
    const struct tcp_sock *tp = tcp_sk(sk);
    struct tcp_wcc *ca = inet_csk_ca(sk);

    u32 new_cwnd;
    new_cwnd = tp->snd_cwnd - divide((tp->snd_cwnd >> 1U), ca->weight);

    return max(new_cwnd, 2U);
}
\end{lstlisting}
观察上述代码可发现，只需自己用整数运算操作实现一个除法函数，然后在\texttt{tp->snd\_cwnd}减半后使其除以权重参数\texttt{ca->weight}即可实现对MD部分的修改。

\begin{figure*}[ht!]
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.2in]{AI-Testbed-Trace.pdf}
    \caption{WCC-AI拟真实验流带宽动态图（使用DropTail队列和选择ACK）}
    \label{fig:AI-TestbedTrace}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.2in]{AI-Testbed-Summary.pdf}
    \caption{WCC-AI拟真实验流总带宽动统计图（使用DropTail队列和选择ACK）}
    \label{fig:AI-TestbedSum}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.2in]{RED-AI-Testbed-Trace.pdf}
    \caption{WCC-AI拟真实验流带宽动态图（使用RED队列和选择ACK）}
    \label{fig:RED-AI-TestbedTrace}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.2in]{RED-AI-Testbed-Summary.pdf}
    \caption{WCC-AI拟真实验流总带宽动统计图（使用RED队列和选择ACK）}
    \label{fig:RED-AI-TestbedSum}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=2.2in]{CACK-AI-Testbed-Trace.pdf}
    \caption{WCC-AI拟真实验流带宽动态图（使用DropTail队列和累积ACK）}
		\label{fig:CACK-AI-TestbedTrace}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=2.2in]{CACK-AI-Testbed-Summary.pdf}
    \caption{WCC-AI拟真实验流总带宽动统计图（使用DropTail队列和累积ACK）}
		\label{fig:CACK-AI-TestbedSum}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=2.2in]{RED-CACK-AI-Testbed-Trace.pdf}
    \caption{WCC-AI拟真实验流带宽动态图（使用RED队列和累积ACK）}
		\label{fig:RED-CACK-AI-TestbedTrace}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=2.2in]{RED-CACK-AI-Testbed-Summary.pdf}
    \caption{WCC-AI拟真实验流总带宽动统计图（使用RED队列和累积ACK）}
		\label{fig:RED-CACK-AI-TestbedSum}
  \end{subfigure}
  \caption{两条权重分别为1和2的WCC-AI流的拟真实验结果。尾部丢包（DropTail）队列的上限为2$MBytes$，早期随机丢包（RED）队列的下阈值为1$Mbytes$，上阈值为5$Mbytes$.}
  \label{fig:TestbedExp}
\end{figure*}

\begin{figure}[h]
  \begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{MulTCP-2M-Testbed-Trace.pdf}
    \caption{MulTCP拟真实验流带宽动态图}
    \label{fig:MulTCP-2M-TestbedTrace}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{MulTCP-2M-Testbed-Summary.pdf}
    \caption{MulTCP拟真实验流总带宽动统计图}
    \label{fig:MulTCP-2M-TestbedSum}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{EWTCP-2M-Testbed-Trace.pdf}
    \caption{EWTCP拟真实验流带宽动态图}
    \label{fig:EWTCP-2M-TestbedTrace}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{EWTCP-2M-Testbed-Summary.pdf}
    \caption{EWTCP拟真实验流总带宽动统计图}
    \label{fig:EWTCP-2M-TestbedSum}
  \end{subfigure}
  \caption{两条权重分别为1和2的流使用分别使用MulTCP和EWTCP的拟真实验结果。带宽动态图中的target line是带权拥塞控制算法追求的目标线。}
  \label{fig:PreWorkTrace}
\end{figure}

\textbf{带权拥塞控制算法在拟真实验环境中也可以实现加权公平性：}以WCC-AI为例，从图\ref{fig:TestbedExp}中的拟真实验结果可以看出，在拟真实验环境中WCC-AI也可以达到近乎完美的加权公平性并在长期中保持着良好的稳定性。
如图\ref{fig:AI-TestbedSum}所示，在整个100s的实验中两条流的长期平均带宽比极其接近其权重比$1:2$。
我还测试了一下早期随机丢包（RED）队列对带权拥塞控制算法的影响。
从图\ref{fig:RED-AI-TestbedTrace}中可以发现，虽然使用RED队列引入了更多的波动，但是WCC-AI在大多数时候也还是能够近似地维持着两条流的加权公平性。
同样地，MulTCP和EWTCP在相同的拟真实验环境中也可以实现差异化带宽分配，但是其加权公平性并不完美。
如图\ref{fig:PreWorkTrace}所示，在流带宽动态图中可以发现，MulTCP和EWTCP各自的两条流的带宽离它们应该达到的目标值都有一定的距离。

\textbf{验证累积ACK确实会对加权公平性产生显著影响：}
我在拟真实验中也测试了累积ACK对加权公平性的影响，结果如图\ref{fig:CACK-AI-TestbedTrace}和\ref{fig:RED-CACK-AI-TestbedTrace}所示。
与图\ref{fig:AI-TestbedTrace}和\ref{fig:RED-AI-TestbedTrace}对比可知，当使用累积ACK选项时，基于AIMD的带权拥塞控制算法的几条TCP流之间的带宽比会显著下降。
原本使用选择ACK时，两条权重分别为1和2的WCC-AI流在仿真总时长内的平均带宽比在使用上限为2$MBytes$的尾部丢包队列时会达到$1:1.99$，在使用上阈值为5$MBytes$、下阈值为1$MBytes$的早期随机丢包队列时会达到$1:1.74$。
当使用累积ACK时，两条权重分别为1和2的WCC-AI流在仿真总时长内的平均带宽比在使用上限为2$MBytes$的尾部丢包队列时就降到了$1:1.56$，在使用上阈值为5$MBytes$、下阈值为1$MBytes$的早期随机丢包队列时就降到了$1.50$。
也就是说权重较大的WCC-AI流的带宽与权重较小的WCC-AI流的带宽相比下降了$\%14~\%25$。

\begin{table}[htp]
	\caption{两条权重分别为1和2的WCC-AI流带宽比随交换机缓冲区大小变化的数据。}
	\label{tab:TrendWithBuffer}
	\centering
	\begin{tabular}{c|cc|c}
		\hline
		缓冲区大小 & \multicolumn{2}{c|}{平均带宽 (Gbps)} & \multirow{2}*{比值}  \\
		\cline{2-3}
		(Bytes) & \multicolumn{1}{c}{Flow 1} & \multicolumn{1}{c|}{Flow 2} \\
		\hline
		12000 K & 3.12 & 6.31 & 1 : 2.022 \\
		5000 K & 3.11 & 6.31 & 1 : 2.029 \\
		2000 K & 3.15 & 6.27 & 1 : 1.990 \\
		1000 K & 3.22 & 6.20 & 1 : 1.925 \\
		500 K & 3.49 & 5.93 & 1 : 1.699 \\
		250 K & 3.69 & 5.61 & 1 : 1.520 \\
		100 K & 3.22 & 5.12 & 1 : 1.590 \\
		500 & 1.37 & 2.04 & 1 : 1.489 \\
		200 & 1.42 & 2.01 & 1 : 1.415 \\
		100 & 1.44 & 1.99 & 1 : 1.382 \\
		10 & 1.36 & 2.02 & 1 : 1.485 \\
		4 & 1.42 & 2.00 & 1 : 1.408 \\
		\hline
	\end{tabular}
\end{table}

\begin{table}[htp]
	\caption{两条权重分别为1和2的WCC-AI流带宽比随链路传播时延变化的数据。}
	\label{tab:TrendWithDelay}
	\centering
	\begin{tabular}{c|cc|c}
		\hline
		拟真  & \multicolumn{2}{c|}{平均带宽 (Gbps)} & \multirow{2}*{比值}  \\
		\cline{2-3}
		时延 & \multicolumn{1}{c}{Flow 1} & \multicolumn{1}{c|}{Flow 2} \\
		\hline
		5 ms & 1.20 & 2.37 & 1 : 1.975 \\
		1 ms & 2.17 & 4.15 & 1 : 1.912 \\
		500 $\mu$s & 2.69 & 5.15 & 1 : 1.914 \\
		375 $\mu$s & 2.88 & 5.20 & 1 : 1.806 \\
		250 $\mu$s & 2.86 & 5.61 & 1 : 1.962 \\
		100 $\mu$s & 3.27 & 5.68 & 1 : 1.737 \\
		50 $\mu$s & 3.42 & 5.80 & 1 : 1.696 \\
		10 $\mu$s & 3.57 & 5.75 & 1 : 1.611 \\
		1 $\mu$s & 3.61 & 5.77 & 1 : 1.598 \\
		\hline
	\end{tabular}
\end{table}


\begin{figure*}[ht]
  \begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{MulTCP-ThroughputVar.pdf}
    \caption{两条MulTCP流带宽随交换机缓冲区大小变化图}
    \label{fig:MulTCP-ThroughputVar}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{MulTCP-RatioVar.pdf}
    \caption{两条MulTCP流带宽比值随交换机缓冲区大小变化图}
    \label{fig:MulTCP-RatioVar}
  \end{subfigure}	
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{EWTCP-ThroughputVar.pdf}
    \caption{两条EWTCP流带宽随交换机缓冲区大小变化图}
    \label{fig:EWTCP-ThroughputVar}
  \end{subfigure}
	\begin{subfigure}{.5\textwidth}
    \centering
		\includegraphics[width=2.3in]{EWTCP-RatioVar.pdf}
    \caption{两条EWTCP流带宽比值随交换机缓冲区大小变化图}
    \label{fig:EWTCP-RatioVar}
  \end{subfigure}
	\caption{权重分别为1和2的两条MulTCP和EWTCP流的带宽及其比值随交换机缓冲区大小变化图。}
	\label{fig:PreWorkVar}
\end{figure*}

\textbf{基于加增乘减的带权拥塞控制算法的加权公平性是会随着网络环境变化而变化的：}
我之前多次提到使用基于加增乘减的带权拥塞控制算法的TCP流会在不同的网络环境配置下获得不同份额的带宽，即若干条流之间的带宽比会随着网络环境的变化而变化。
我最开始是在测试MulTCP和EWTCP时发现这一现象的。
如图\ref{fig:PreWorkVar}所示，当实验中设置的交换机缓冲区大小由10$Kbytes$变化到500$Kbytes$时，两条MulTCP流和EWTCP流之间的带宽比都发生了显著的变化。
对于MulTCP和EWTCP来说，只有当交换机缓冲区的大小比较小时，它们才能使两条权重分别为1和2的TCP流的带宽比达到它们的权重比，不过这也是以牺牲链路带宽利用率为代价的。
而事实上这种现象不是MulTCP和EWTCP的特性，在进一步的实验中，我发现WCC-AI和WCC-MD也会展现出类似的不稳定性。
以WCC-AI为例，在拟真实验中我发现两条权重分别为1和2的WCC-AI流的带宽比会随着交换机缓冲区大小或链路传播时延的不同而发生显著变化。
如表\ref{tab:TrendWithBuffer}所示，当实验中设置的交换机缓冲区大小由12$Mbytes$变化到500$Bytes$时，两条WCC-AI流的带宽比由$1:2.022$降到了$1:1.489$。
除了交换机缓冲区大小，链路传播时延也会影响到基于加增乘减的带权拥塞控制算法的加权公平性。
由于连接服务器与交换机的光缆是固定长度的且由于光缆本身长度不大导致其传播时延极小（约等于16纳秒），所以我使用了Linux Traffic Control中的netem模块来模拟额外的传播时延。
如表\ref{tab:TrendWithDelay}所示，当实验中设置的模拟链路时延由5$ms$变化到1$\mu s$时，两条WCC-AI流的带宽比由$1:1.975$降到了$1:1.598$。
综上所述，我们可以得出基于加增乘减的带权拥塞控制算法的加权公平性确实是会受到网络环境的影响。
更准确地说，当交换机缓冲区大小越大、链路传播时延越高时，基于加增乘减的带权拥塞控制算法给权重不同的流分配的带宽差异越大，因此就会导致其加权公平性发生改变。

\chapter{讨论与启示}\label{chapter:discusssion}

基于仿真和拟真实验的分析说明了，虽然基于加增乘减的带权拥塞控制算法确实可以实现差异化带宽分配，但是其加权公平性还有存在着显著的不稳定性和不可理解性。
对于不稳定性，接收端窗口和ACK选项的影响是比较容易摒除得，只需把接收端窗口设置得足够大和默认开启选择ACK选项即可。
但是来自于网络环境方面的影响则很难消除，交换机缓冲区大小和链路传播时延等环境变量都是经常变化的，这也就意味着使用带权拥塞控制的TCP流在不同的网络环境中获得的带宽份额是会变化的。
而且由于目前还不清楚带权拥塞控制算法的加权公平性随网络环境变化的规律，所以带权拥塞控制算法在实际应用中很难精确地按流的权重分配给TCP流分配带宽。
这也意味着想要在实际环境中良好地利用带权拥塞控制算法，那我们首先要做的就是弄清带权拥塞控制算法的加权公平性随网络环境变化的基本规律。
比如，在实验数据中我们可以发现带权拥塞控制算法的加权公平性还是存在稳定点的，如表\ref{tab:TrendWithBuffer}中所示，当交换机缓冲区大小大于2$Mbytes$时，两条流的权重比基本上就维持在$1:2$附近了。

如果我们能弄清带权拥塞控制算法的加权公平性随网络环境变化的基本规律，那么我们就有可能精确地找到带权拥塞控制算法的稳定点，进而我们就可以努力使带权拥塞控制算法运行在稳定状态中或者我们还可以开发出能够在特定环境下保持稳定的带权拥塞控制算法。
这也就牵涉到了带权拥塞控制算法的可理解性问题，而目前带权拥塞控制算法由于缺乏合适的理论模型，其可理解性都是很差的。
我们在实验和分析中发现的MulTCP和EWTCP的加权公平性随交换机缓冲区大小变化的规律在它们的理论模型\cite{crowcroft1998differentiated}\cite{padhye1998modeling}中都没有能够反映出来。
近几年流行的流模型\cite{alizadeh2011analysis}\cite{zhu2015congestion}也没能反映出这种规律。
对二项式拥塞控制算法进行通用化建模的模型\cite{bansal2001binomial}也没能刻画这种变化规律。
因此，可以说现有的普通TCP流带宽建模模型都无法直接用于对使用带权拥塞控制的TCP流进行建模。

这些现有的TCP流建模模型都是针对单条流进行建模，并最终生成一个含有几个网络环境参数的TCP流带宽公式。
而对于带权拥塞控制算法，我们其实并不关注某条流的具体带宽，而是更为关注所有参与竞争的流之间的带宽比。
因此从本质上来说，现有的TCP流建模模型都不能很好地切合带权拥塞控制算法建模的目标。
而这也就启发我们：我们也许能够通过创造新的融合多条TCP流的建模模型，来捕捉带权拥塞控制算法的加权公平性随环境变化的规律。
我相信如果研究者们能够创造一种综合多个网络环境参数并对多条TCP流的带宽比进行建模的模型，那么我们就将能够掌握带权拥塞控制算法的加权公平性随网络环境变化的基本规律。

\chapter{相关工作}\label{chapter:related works}

{\bf 控制策略：} 
在同步反馈的假设下，Chiu和Jain分析了加增乘减（AIMD）、加增加减（AIAD）、乘增加减（MIAD）和乘增乘减（MIMD）等四种经典控制策略的高效性和公平性，并总结到只有AIMD能够在稳态时同时实现高效性和公平性\cite{chiu1989analysis}。
此后，人们基于加增乘减的控制策略提出了大量的拥塞控制算法，并使得拥塞控制逐渐走向成熟，这其中就包含RAP~\cite{rejaie1999rap}， RLM~\cite{mccanne1996rlm}， LDA~\cite{sisalem1998lda}， Reno~\cite{jacobson1990modified}， Vegas~\cite{brakmo1995tcp}， NewReno~\cite{Henderson2012NewReno}等经典拥塞控制算法。
当基于加增乘减的拥塞控制成熟以后，研究者们提出了通用加增乘减（General AIMD）\cite{yang2000general}和二项式拥塞控制（Binomial Congestion Control）\cite{bansal2001binomial}，希望以非线性的控制模型来泛化线性的控制策略。
特别地，两种TCP友好的非线性控制策略——IIAD和SQRT被设计出来以用于实时流式应用\cite{bansal2001binomial}。
General AIMD\cite{yang2000general}也阐述了类似的想法，但它主要关注AIMD的泛化。
之后，还有人提出了使用SIMD——一种使用超线性增长策略和历史信息的拥塞控制——来提升收敛时间。

{\bf 拥塞控制: }
TCP一般利用丢包\cite{jacobson1990modified,Henderson2012NewReno,ha2008cubic}、往返时间变化\cite{brakmo1995tcp,cardwell2016bbr,arun2018copa,Zeng2019cc}等网络信号来对潜在的网络拥塞做出响应。
DCTCP\cite{alizadeh2010data,alizadeh2011analysis}通过在乘减阶段结合ECN机制，为数据中心网络提供了一种传输速率更为平滑的拥塞控制算法。
为了解决新型高性能网络中的拥塞问题，人们近年来提出了DCQCN\cite{zhu2015congestion}、Timely\cite{mittal2015timely}和HPCC\cite{li2019hpcc}三种新型的低时延拥塞控制算法。
其中，DCQCN结合了QCN\cite{pan2007qcn}和DCTCP\cite{alizadeh2010data}的优点，Timely结合了传统的基于时延的拥塞控制算法思想和新型网卡的特性，HPCC则充分利用了新型可编程交换机的功能。
最近，还有人提出了使用时下最热的机器学习算法来实现细粒度的拥塞控制，其中的典型代表是Remy\cite{winstein2013remy}和PCC Vivace\cite{dong2018pcc}。
虽然这么多年来，人们不断地提出了上述种种日益新奇的拥塞控制算法，但是这些工作都是在追求公平性以及高带宽和低时延。
它们都没有关注过应该如何实现拥塞控制算法的加权公平性。

{\bf 加权公平性: }
公平性是一个历史悠久的概念，在其之下还有许多不同的具体子概念。
Max-min Fairness\cite{bertsekas1992data,jaffe1981bottleneck}和Proportional Fairness\cite{kelly1998rate}就是最著名的两种具体定义的公平性，它们还可以拓展到一个更普适的公平性定义——$\alpha$-fairness\cite{mo2000fair}。
%These works all present fundamental theories about how to share network bandwidth resources.
%Nevertheless, though they provide some guides for designing bandwidth sharing policy of weighted congestion control, they don't give any analysis and solution about how to implement them.
随着网络由free-use模型演化到pay-for-use模型，公平性原则便逐渐被拓展到了加权公平性，即在“公平地”分配资源时要参照各个竞争者的权重。
MulTCP\cite{crowcroft1998differentiated}和EWTCP\cite{Honda2009EWTCP}均试图通过修改AIMD的具体行为来实现流级的加权公平性。
Faircloud\cite{popa2012faircloud}尝试基于Weighted Fair Queuing\cite{demers1989analysis,Abhay1993WFQ}来实现网络级的加权公平性
NUMFabric\cite{nagaraj2016numfabric}和DGD\cite{low1999DGD}则设计了复杂的、需要特定交换机支持的分布式优化算法，以实现包括加权公平性在内的一般化的网络带宽分配。
一份最近的工作Minerva\cite{Nathan2019wcubic}基于CUBIC\cite{ha2008cubic}和FAST\cite{chen2004fast}实现了带权拥塞控制，并希望利用它来实现用户体验质量的公平性。
此外，Multipath TCP\cite{wischik2011design,lu2018multi}也提到了需要使用带权拥塞控制来控制细粒度的带宽分配。

\chapter{总结}\label{chapter:conclusion}

在本文中，我对包括MulTCP和EWTCP在内的四种基于加增乘减的带权拥塞控制算法进行了仿真和拟真实验，并分析了它们的一些特性，揭示了它们在实际网络环境中的不稳定性。
我还进一步地总结了带权拥塞控制算法的加权公平性与环境变量之间的关系，指明带权拥塞控制算法的加权公平性会随着交换机缓冲区大小、链路传播时延等环境因素的变化而变化。
最后，我在讨论中分析了现有TCP流带宽建模模型存在的问题，并提出下一步的研究方向在于创造新的针对多条TCP流带宽比建模的模型。

\bibliography{reference}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 致谢，应放在结论之后
\begin{acknowledgement}
我非常怀念在NASA实验室度过的两年时光，老师们无论在学术还是人生的指导上都对我起到了很大的帮助，师兄师姐小伙伴们的鼓励、支持和陪伴也促使我不断成长。感谢田臣老师和郑嘉琦老师的真诚帮助！
\end{acknowledgement}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
